{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DreRnc/ExplainingExplanations/blob/main/Explainations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H972d7y9V7J"
      },
      "source": [
        "Dataset : **E-SNLI**. \\\n",
        "Model : **Small T5**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# git clone https://github.com/DreRnc/ExplainingExplanations.git\n",
        "# cd ExplainingExplanations\n",
        "# pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.0 Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0kOOUYl19UHq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dre/Desktop/GitRepo/DreRnc/ExplainingExplanations/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"esnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training_set:  (549367, 6)\n",
            "Shae of validation_set:  (9842, 6)\n",
            "Shape of test_set:  (9824, 6)\n"
          ]
        }
      ],
      "source": [
        "training_set = dataset['train']\n",
        "validation_set = dataset['validation']\n",
        "test_set = dataset['test']\n",
        "\n",
        "print(\"Shape of training_set: \", training_set.shape)\n",
        "print(\"Shae of validation_set: \", validation_set.shape)\n",
        "print(\"Shape of test_set: \", test_set.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'premise': 'A person on a horse jumps over a broken down airplane.',\n",
              " 'hypothesis': 'A person is training his horse for a competition.',\n",
              " 'label': 1,\n",
              " 'explanation_1': 'the person is not necessarily training his horse',\n",
              " 'explanation_2': '',\n",
              " 'explanation_3': ''}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train_small:  (5000, 6)\n",
            "Shape of valid_small:  (5000, 6)\n",
            "Shape of test_small:  (5000, 6)\n"
          ]
        }
      ],
      "source": [
        "n_train = n_valid = n_test = 5000\n",
        "\n",
        "train_small = training_set.select(range(n_train))\n",
        "valid_small = validation_set.select(range(n_valid))\n",
        "test_small = test_set.select(range(n_test))\n",
        "\n",
        "print(\"Shape of train_small: \", train_small.shape)\n",
        "print(\"Shape of valid_small: \", valid_small.shape)\n",
        "print(\"Shape of test_small: \", test_small.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Loading T5 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jyczhxOR-jwU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1nc40MHGQQx"
      },
      "source": [
        "Test **zero-shot** on a random task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKG-so0--7ug",
        "outputId": "38dfed95-b25b-4e87-9276-94e555ffeaae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour Dre, je pense que la version anglaise est bonne pour nous.\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(\"translate English to French: Hello Dre, I think the English version is ok for us.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids,  max_new_tokens = 100)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True, max_length = 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Zero-shot to Verify Everything is Working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import generate_prompt_mnli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZILJcgoDO9b",
        "outputId": "0be6311d-1473-47db-c9e2-fed580dca801"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'premise': 'A person on a horse jumps over a broken down airplane.',\n",
              " 'hypothesis': 'A person is training his horse for a competition.',\n",
              " 'label': 1,\n",
              " 'explanation_1': 'the person is not necessarily training his horse',\n",
              " 'explanation_2': '',\n",
              " 'explanation_3': ''}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = training_set[0]\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating the prompt:\n",
        "\n",
        "<b><u> mnli hypothesis: </b></u> The St. Louis Cardinals have always won. <b><u> premise: </b></u> yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but\n",
        "\n",
        "Output: \n",
        "* 0: Entailment \n",
        "* 1: Neutral\n",
        "* 2: Contradiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7D2MQG6qJYnI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mnli hypothesis: A person is training his horse for a competition. premise: A person on a horse jumps over a broken down airplane.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = generate_prompt_mnli(example)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1roa6HcIi3e",
        "outputId": "521862a6-40d6-4979-f9ed-34071eac41f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neutral\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dre/Desktop/GitRepo/DreRnc/ExplainingExplanations/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(prompt, return_tensors= \"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.0 Task 1: Zero-shot evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils import evaluate_output_mnli\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [04:51<00:00, 17.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy zero shot on test set:  0.717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "seen = 0\n",
        "\n",
        "for datapoint in tqdm(test_small):\n",
        "    prompt = generate_prompt_mnli(datapoint)\n",
        "    input_ids = tokenizer(prompt, return_tensors= \"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    correct += evaluate_output_mnli(output, datapoint['label'])\n",
        "\n",
        "print(\"Accuracy zero shot on test set: \", correct/n_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.0 Task 2: Fine tuning without explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.0 Task 4: Making the model generate explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOD1t8GCLbe50z4YUG+KN3k",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
